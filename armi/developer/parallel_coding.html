<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />
<meta property="og:title" content="5. Parallel Code in ARMI" />
<meta property="og:type" content="website" />
<meta property="og:url" content="https://terrapower.github.io/armi/developer/parallel_coding.html" />
<meta property="og:site_name" content="Advanced Reactor Modeling Interface" />
<meta property="og:description" content="ARMI simulations can be parallelized using the mpi4py module. You should go there and read about collective and point-to-point communication if you want to understand everything in-depth. The OS-le..." />
<meta property="og:image" content="https://terrapower.github.io/armi/_static/armiSchematicView.png" />
<meta property="og:image:alt" content="Advanced Reactor Modeling Interface" />
<meta name="description" content="ARMI simulations can be parallelized using the mpi4py module. You should go there and read about collective and point-to-point communication if you want to understand everything in-depth. The OS-le..." />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>5. Parallel Code in ARMI &mdash; ARMI 0.3.0 documentation</title>
      <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../_static/css/theme_fixes.css" type="text/css" />
      <link rel="stylesheet" href="../_static/graphviz.css" type="text/css" />
      <link rel="stylesheet" href="../_static/sg_gallery.css" type="text/css" />
      <link rel="stylesheet" href="../_static/sg_gallery-binder.css" type="text/css" />
      <link rel="stylesheet" href="../_static/sg_gallery-dataframe.css" type="text/css" />
      <link rel="stylesheet" href="../_static/sg_gallery-rendered-html.css" type="text/css" />
    <link rel="shortcut icon" href="../_static/armiicon_16x16.ico"/>
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/sphinx_highlight.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="6. Profiling ARMI" href="profiling.html" />
    <link rel="prev" title="4. Documenting ARMI" href="documenting.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search"  style="background: #233C5B" >

          
          
          <a href="../index.html" class="icon icon-home">
            ARMI
              <img src="../_static/armiicon_24x24.ico" class="logo" alt="Logo"/>
          </a>
              <div class="version">
                0.3.0
              </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../user/index.html">User Docs</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="index.html">Developer Docs</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="guide.html">1. Framework Architecture</a></li>
<li class="toctree-l2"><a class="reference internal" href="making_armi_based_apps.html">2. Making ARMI-based Apps</a></li>
<li class="toctree-l2"><a class="reference internal" href="entrypoints.html">3. Entry Points</a></li>
<li class="toctree-l2"><a class="reference internal" href="documenting.html">4. Documenting ARMI</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">5. Parallel Code in ARMI</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#mpi-communication-crash-course">5.1. MPI communication crash course</a></li>
<li class="toctree-l3"><a class="reference internal" href="#mpi-communication-within-armi">5.2. MPI Communication within ARMI</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#example-using-bcast">5.2.1. Example using <code class="docutils literal notranslate"><span class="pre">bcast</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#example-using-scatter">5.2.2. Example using <code class="docutils literal notranslate"><span class="pre">scatter</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#a-simplified-approach">5.2.3. A simplified approach</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="profiling.html">6. Profiling ARMI</a></li>
<li class="toctree-l2"><a class="reference internal" href="reports.html">7. Reports in ARMI</a></li>
<li class="toctree-l2"><a class="reference internal" href="first_time_contributors.html">8. First Time Contributors Guide</a></li>
<li class="toctree-l2"><a class="reference internal" href="standards_and_practices.html">9. Standards and Practices for Coding</a></li>
<li class="toctree-l2"><a class="reference internal" href="tooling.html">10. Tooling and Infrastructure</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../gallery/index.html">Gallery</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorials/index.html">Tutorials</a></li>
<li class="toctree-l1"><a class="reference internal" href="../release/index.html">Release Notes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../glossary.html">Glossary</a></li>
<li class="toctree-l1"><a class="reference internal" href="../.apidocs/modules.html">API Docs</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu"  style="background: #233C5B" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">ARMI</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content style-external-links">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="index.html">Developer Docs</a></li>
      <li class="breadcrumb-item active"><span class="section-number">5. </span>Parallel Code in ARMI</li>
  <li class="wy-breadcrumbs-aside">
    
      
    
  </li>

  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="parallel-code-in-armi">
<h1><span class="section-number">5. </span>Parallel Code in ARMI<a class="headerlink" href="#parallel-code-in-armi" title="Permalink to this heading"></a></h1>
<p>ARMI simulations can be parallelized using the <a class="reference external" href="https://mpi4py.readthedocs.io/en/stable/mpi4py.html">mpi4py</a>
module. You should go there and read about collective and point-to-point communication if you want to
understand everything in-depth.</p>
<p>The OS-level <code class="docutils literal notranslate"><span class="pre">mpiexec</span></code> command is used to run ARMI on, say, 10 parallel processors. This fires up 10 identical
and independent runs of ARMI; they do not share memory. If you change the reactor on one process, the reactors
don’t change on the others.</p>
<p>Never fear. You can communicate between these processes using the Message Passing Interface (MPI) driver
via the Python <code class="docutils literal notranslate"><span class="pre">mpi4py</span></code> module. In fact, ARMI is set up to do a lot of the MPI work for you, so if you follow
these instructions, you can have your code working in parallel in no time. In ARMI, there’s the primary processor
(which is the one that does most of the organization) and then there are the worker processors, which do whatever
you need them to in parallel.</p>
<section id="mpi-communication-crash-course">
<h2><span class="section-number">5.1. </span>MPI communication crash course<a class="headerlink" href="#mpi-communication-crash-course" title="Permalink to this heading"></a></h2>
<p>First, let’s do a crash course in MPI communications. We’ll only discuss a few important ideas, you can read
about more on the <code class="docutils literal notranslate"><span class="pre">mpi4py</span></code> web page. The first method of communication is called the <code class="docutils literal notranslate"><span class="pre">broadcast</span></code>, which
happens when the primary processor sends information to all others. An example of this would be when you want to
sync up the settings object (<code class="docutils literal notranslate"><span class="pre">self.cs</span></code>) among all processors. An even more common example is when you want to
send a simple string command to all other processors. This is used all the time to inform the workers what they
are expected to do next.</p>
<p>Here is an example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">armi</span> <span class="kn">import</span> <span class="n">context</span>

<span class="n">cmd</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;val</span><span class="si">{</span><span class="n">context</span><span class="o">.</span><span class="n">MPI_RANK</span><span class="si">}</span><span class="s2">&quot;</span>

<span class="k">if</span> <span class="n">context</span><span class="o">.</span><span class="n">MPI_RANK</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
    <span class="c1"># The primary node will send the string &#39;bob&#39; to all others</span>
    <span class="n">cmd</span> <span class="o">=</span> <span class="s2">&quot;bob&quot;</span>
    <span class="n">context</span><span class="o">.</span><span class="n">MPI_COMM</span><span class="o">.</span><span class="n">bcast</span><span class="p">(</span><span class="n">cmd</span><span class="p">,</span> <span class="n">root</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
    <span class="c1"># these are the workers. They receive a value and set it to the variable cmd</span>
    <span class="n">context</span><span class="o">.</span><span class="n">MPI_COMM</span> <span class="o">=</span> <span class="n">comm</span><span class="o">.</span><span class="n">bcast</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">root</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
<p>Note that the <code class="docutils literal notranslate"><span class="pre">comm</span></code> object is from the <code class="docutils literal notranslate"><span class="pre">mpi4py</span></code> module that deals with the MPI drivers. The value of cmd on
the worker before and after the <code class="docutils literal notranslate"><span class="pre">bcast</span></code> command are shown in the table.</p>
<table class="docutils align-default">
<tbody>
<tr class="row-odd"><td></td>
<td><p>Proc0</p></td>
<td><p>Proc1</p></td>
<td><p>Proc2</p></td>
<td><p>Proc3</p></td>
</tr>
<tr class="row-even"><td><p>Before bcast</p></td>
<td><p>“bob”</p></td>
<td><p>“val1”</p></td>
<td><p>“val2”</p></td>
<td><p>“val3”</p></td>
</tr>
<tr class="row-odd"><td><p>After bcast</p></td>
<td><p>“bob”</p></td>
<td><p>“bob”</p></td>
<td><p>“bob”</p></td>
<td><p>“bob”</p></td>
</tr>
</tbody>
</table>
<p>The second important type of communication is the <code class="docutils literal notranslate"><span class="pre">scatter</span></code>/<code class="docutils literal notranslate"><span class="pre">gather</span></code> combo. These are used when you have a
big list of work you’d like to get done in parallel and you want to farm it off to a bunch of processors. To do
this, set up a big list of work to get done on the primary. Some real examples are that the list contains things
like run control parameters, assemblies, or blocks. For a trivial example, let’s add a bunch of values in parallel.
First, let’s create 1000 random numbers to add:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">random</span>
<span class="n">workList</span> <span class="o">=</span> <span class="p">[(</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">(),</span> <span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">())</span> <span class="k">for</span> <span class="n">_i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1000</span><span class="p">)]</span>
</pre></div>
</div>
<p>Now we want to distribute this work to each of the worker processors (and take one for the primary too, so it’s
not just sitting around waiting). This is what <code class="docutils literal notranslate"><span class="pre">scatter</span></code> will do. But <code class="docutils literal notranslate"><span class="pre">scatter</span></code> requires a list that has
length exactly equal to the number of processors available. You have some options here. Assuming there are 10
CPUs, you can either pass the first 10 values out of the list and keep sending groups of  10 values until they
are all sent (multiple sets of transmitions) or you can split the data up into 10 evenly-populated groups (single
transmition to each CPU). This is called <em>load balancing</em>.</p>
<p>ARMI has utilities that can help called <a class="reference internal" href="../.apidocs/armi.utils.iterables.html#armi.utils.iterables.chunk" title="armi.utils.iterables.chunk"><code class="xref py py-func docutils literal notranslate"><span class="pre">armi.utils.iterables.chunk()</span></code></a> and <a class="reference internal" href="../.apidocs/armi.utils.iterables.html#armi.utils.iterables.flatten" title="armi.utils.iterables.flatten"><code class="xref py py-func docutils literal notranslate"><span class="pre">armi.utils.iterables.flatten()</span></code></a>.
Given an arbitrary list, <code class="docutils literal notranslate"><span class="pre">chunk</span></code> breaks it up into a certain number of chunks and <code class="docutils literal notranslate"><span class="pre">unchunk</span></code> does the
opposite to reassemble the original list after processing. Let’s look at an example script:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="sd">&quot;&quot;&quot;mpi_example.py&quot;&quot;&quot;</span>
<span class="kn">import</span> <span class="nn">random</span>

<span class="kn">from</span> <span class="nn">armi</span> <span class="kn">import</span> <span class="n">context</span>
<span class="kn">from</span> <span class="nn">armi.utils</span> <span class="kn">import</span> <span class="n">iterables</span>

<span class="c1"># Generate a list of random number pairs: [[(v1,v2),(v3,v4),...]]</span>
<span class="n">workList</span> <span class="o">=</span> <span class="p">[(</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">(),</span> <span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">())</span> <span class="k">for</span> <span class="n">_i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1000</span><span class="p">)]</span>

<span class="k">if</span> <span class="n">context</span><span class="o">.</span><span class="n">MPI_RANK</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
    <span class="c1"># Primary Process: Split the data and send it to the workers</span>
    <span class="n">workListLoadBalanced</span> <span class="o">=</span> <span class="n">iterables</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">workList</span><span class="p">,</span> <span class="n">context</span><span class="o">.</span><span class="n">MPI_SIZE</span><span class="p">,</span> <span class="n">padWith</span><span class="o">=</span><span class="p">())</span>
    <span class="n">myValsToAdd</span> <span class="o">=</span> <span class="n">context</span><span class="o">.</span><span class="n">MPI_COMM</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">workListLoadBalanced</span><span class="p">,</span> <span class="n">root</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
    <span class="c1"># Worker Process: Receive data, pass a dummy value to scatter (None)</span>
    <span class="n">myValsToAdd</span> <span class="o">=</span> <span class="n">context</span><span class="o">.</span><span class="n">MPI_COMM</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">root</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>


<span class="c1"># All processes do their bit of this work (adding)</span>
<span class="n">results</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">num1</span><span class="p">,</span> <span class="n">num2</span> <span class="ow">in</span> <span class="n">myValsToAdd</span><span class="p">:</span>
    <span class="n">results</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">num1</span> <span class="o">+</span> <span class="n">num2</span><span class="p">)</span>

<span class="c1"># All processes call gather to send their results back to the root process.</span>
<span class="c1">#    (The result lists above are simply added to make one list with MPI_SIZE sub-lists.)</span>
<span class="n">allResultsLoadBalanced</span> <span class="o">=</span> <span class="n">context</span><span class="o">.</span><span class="n">MPI_COMM</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="n">results</span><span class="p">,</span> <span class="n">root</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="c1"># Primary Process: Flatten the multiple lists (from each process), and sum them.</span>
<span class="k">if</span> <span class="n">context</span><span class="o">.</span><span class="n">MPI_RANK</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
    <span class="c1"># Flatten the MPI_SIZE number of sub lists into one list</span>
    <span class="n">allResults</span> <span class="o">=</span> <span class="n">iterables</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">allResultsLoadBalanced</span><span class="p">)</span>
    <span class="c1"># Sum the final list, and print the result</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;The total sum is: </span><span class="si">{0:10.5f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">sum</span><span class="p">(</span><span class="n">allResults</span><span class="p">)))</span>
</pre></div>
</div>
<p>Remember that this code is running on all processors. So it’s just the <code class="docutils literal notranslate"><span class="pre">if</span> <span class="pre">rank</span> <span class="pre">==</span> <span class="pre">0</span></code> statements that differentiate between the primary and the workers. To really understand what this script is doing, try to run it in parallel and see what it prints out:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">mpiexec</span> <span class="o">-</span><span class="n">n</span> <span class="mi">4</span> <span class="n">python</span> <span class="n">mpi_example</span><span class="o">.</span><span class="n">py</span>
</pre></div>
</div>
</section>
<section id="mpi-communication-within-armi">
<h2><span class="section-number">5.2. </span>MPI Communication within ARMI<a class="headerlink" href="#mpi-communication-within-armi" title="Permalink to this heading"></a></h2>
<p>Now that you understand the basics, here’s how you should get your <a class="reference internal" href="../.apidocs/armi.interfaces.html#armi.interfaces.Interface" title="armi.interfaces.Interface"><code class="xref py py-class docutils literal notranslate"><span class="pre">armi.interfaces.Interface</span></code></a>
to run things in parallel in ARMI.</p>
<p>You don’t have to worry too much about the ranks, etc. because ARMI will set that up for you. Basically,
the interfaces are executed by the primary node unless you say otherwise. All workers are stalled in an <code class="docutils literal notranslate"><span class="pre">MPI.bcast</span></code> waiting
for your command! The best coding practice is to create an <a class="reference internal" href="../.apidocs/armi.mpiActions.html#armi.mpiActions.MpiAction" title="armi.mpiActions.MpiAction"><code class="xref py py-class docutils literal notranslate"><span class="pre">MpiAction</span></code></a> subclass and override
the <a class="reference internal" href="../.apidocs/armi.mpiActions.html#armi.mpiActions.MpiAction.invokeHook" title="armi.mpiActions.MpiAction.invokeHook"><code class="xref py py-meth docutils literal notranslate"><span class="pre">invokeHook()</span></code></a> method. <cite>MpiActions</cite> can be broadcast, gathered, etc. and within
the <a class="reference internal" href="../.apidocs/armi.mpiActions.html#armi.mpiActions.MpiAction.invokeHook" title="armi.mpiActions.MpiAction.invokeHook"><code class="xref py py-meth docutils literal notranslate"><span class="pre">invokeHook()</span></code></a> method have <code class="docutils literal notranslate"><span class="pre">o</span></code>, <code class="docutils literal notranslate"><span class="pre">r</span></code>, and <code class="docutils literal notranslate"><span class="pre">cs</span></code> attributes.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>When communicating raw Blocks or Assemblies all references to parents are lost. If a whole reactor is needed
use <code class="docutils literal notranslate"><span class="pre">DistributeStateAction</span></code> and <code class="docutils literal notranslate"><span class="pre">syncMpiState</span></code> (shown in last example).  Additionally, note that if a <code class="docutils literal notranslate"><span class="pre">self.r</span></code>
exists on the <code class="docutils literal notranslate"><span class="pre">MpiAction</span></code> prior to transmission it will be removed when <code class="docutils literal notranslate"><span class="pre">invoke()</span></code> is called.</p>
</div>
<p>If you have a bunch of blocks that you need independent work done on, always remember that unless you explicitly
MPI transmit the results, they will not survive on the primary node. For instance, if each CPU computes and sets
a block parameter (e.g. <code class="docutils literal notranslate"><span class="pre">b.p.paramName</span> <span class="pre">=</span> <span class="pre">10.0)</span></code>, these <strong>will not</strong> be set on the primary! There are a few
mechanisms that can help you get the data back to the primary reactor.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If you want similar capabilities for objects that are not blocks, take another look at <a class="reference internal" href="../.apidocs/armi.utils.iterables.html#armi.utils.iterables.chunk" title="armi.utils.iterables.chunk"><code class="xref py py-func docutils literal notranslate"><span class="pre">armi.utils.iterables.chunk()</span></code></a>.</p>
</div>
<section id="example-using-bcast">
<h3><span class="section-number">5.2.1. </span>Example using <code class="docutils literal notranslate"><span class="pre">bcast</span></code><a class="headerlink" href="#example-using-bcast" title="Permalink to this heading"></a></h3>
<p>Some actions that perform the same task are best distributed through a broadcast. This makes sense for if your are
parallelizing code that is a function of an individual assembly, or block. In the following example, the interface simply
creates an <code class="docutils literal notranslate"><span class="pre">Action</span></code> and broadcasts it as appropriate:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">armi</span> <span class="kn">import</span> <span class="n">context</span>

<span class="k">class</span> <span class="nc">SomeInterface</span><span class="p">(</span><span class="n">interfaces</span><span class="o">.</span><span class="n">Interface</span><span class="p">):</span>

    <span class="k">def</span> <span class="nf">interactEverNode</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">cycle</span><span class="p">,</span> <span class="n">node</span><span class="p">):</span>
        <span class="n">action</span> <span class="o">=</span> <span class="n">BcastAction</span><span class="p">()</span>
        <span class="n">context</span><span class="o">.</span><span class="n">MPI_COMM</span><span class="o">.</span><span class="n">bcast</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>
        <span class="n">results</span> <span class="o">=</span> <span class="n">action</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">o</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">r</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cs</span><span class="p">)</span>

        <span class="c1"># allResults is a list of len(self.r)</span>
        <span class="k">for</span> <span class="n">aResult</span> <span class="ow">in</span> <span class="n">results</span><span class="p">:</span>
            <span class="n">a</span><span class="o">.</span><span class="n">p</span><span class="o">.</span><span class="n">someParam</span> <span class="o">=</span> <span class="n">aResult</span>

<span class="k">class</span> <span class="nc">BcastAction</span><span class="p">(</span><span class="n">mpiActions</span><span class="o">.</span><span class="n">MpiAction</span><span class="p">):</span>

    <span class="k">def</span> <span class="nf">invokeHook</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># do something with the local self.r, self.o, and self.cs.</span>
        <span class="c1"># in this example... do stuff for assemblies.</span>
        <span class="n">results</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">a</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">mpiIter</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">r</span><span class="p">):</span>
            <span class="n">results</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">someFunction</span><span class="p">(</span><span class="n">a</span><span class="p">))</span>

        <span class="c1"># in this usage, it makes sense to gather the results</span>
        <span class="n">allResults</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="n">results</span><span class="p">)</span>

        <span class="c1"># Only primary node has allResults</span>
        <span class="k">if</span> <span class="n">allResults</span><span class="p">:</span>
            <span class="c1"># Flatten results returns the original order after having</span>
            <span class="c1"># made lists of mpiIter results.</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">mpiFlatten</span><span class="p">(</span><span class="n">allResults</span><span class="p">)</span>
</pre></div>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Currently, there is no guarantee that the reactor state is the same across all nodes. Consequently, the above code
should really contain a <code class="docutils literal notranslate"><span class="pre">mpiActions.DistributeStateAction.invokeAsMaster</span></code> call prior to broadcasting the
<code class="docutils literal notranslate"><span class="pre">action</span></code>. See example below.</p>
</div>
</section>
<section id="example-using-scatter">
<h3><span class="section-number">5.2.2. </span>Example using <code class="docutils literal notranslate"><span class="pre">scatter</span></code><a class="headerlink" href="#example-using-scatter" title="Permalink to this heading"></a></h3>
<p>When trying two independent actions at the same time, you can use <code class="docutils literal notranslate"><span class="pre">scatter</span></code> to distribute the work. The following example
shows how different operations can be performed in parallel:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">SomeInterface</span><span class="p">(</span><span class="n">interfaces</span><span class="o">.</span><span class="n">Interface</span><span class="p">):</span>

    <span class="k">def</span> <span class="nf">interactEveryNode</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">cycle</span><span class="p">,</span> <span class="n">node</span><span class="p">):</span>
        <span class="n">actions</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="c1"># pseudo code for getting a bunch of different actions</span>
        <span class="k">for</span> <span class="n">opt</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">cs</span><span class="p">[</span><span class="s1">&#39;someSetting&#39;</span><span class="p">]:</span>
            <span class="n">actions</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">factory</span><span class="p">(</span><span class="n">opt</span><span class="p">))</span>

        <span class="n">distrib</span> <span class="o">=</span> <span class="n">mpiActions</span><span class="o">.</span><span class="n">DistributeStateAction</span><span class="p">()</span>
        <span class="n">distrib</span><span class="o">.</span><span class="n">broadcast</span><span class="p">()</span>

        <span class="c1"># this line any existing reactor on workers to ensure consistency</span>
        <span class="n">distrib</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">o</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">r</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cs</span><span class="p">)</span>
        <span class="c1"># the 3 lines above are equivalent to:</span>
        <span class="c1"># mpiActions.DistributeStateAction.invokeAsMaster(self.o, self.r, self.cs)</span>

        <span class="n">results</span> <span class="o">=</span> <span class="n">mpiActions</span><span class="o">.</span><span class="n">runActions</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">o</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">r</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cs</span><span class="p">,</span> <span class="n">actions</span><span class="p">)</span>

        <span class="c1"># do something to apply the results.</span>
        <span class="k">for</span> <span class="n">bi</span><span class="p">,</span> <span class="n">b</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">r</span><span class="o">.</span><span class="n">getBlocks</span><span class="p">():</span>
            <span class="n">b</span><span class="o">.</span><span class="n">p</span><span class="o">.</span><span class="n">what</span> <span class="o">=</span> <span class="n">extractBlockResult</span><span class="p">(</span><span class="n">results</span><span class="p">,</span> <span class="n">bi</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">factory</span><span class="p">(</span><span class="n">opt</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">opt</span> <span class="o">==</span> <span class="s1">&#39;WHAT&#39;</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">WhatAction</span><span class="p">()</span>

<span class="k">class</span> <span class="nc">WhatAction</span><span class="p">(</span><span class="n">mpiActions</span><span class="o">.</span><span class="n">MpiAction</span><span class="p">):</span>

    <span class="k">def</span> <span class="nf">invokeHook</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># does something</span>
        <span class="c1"># somehow gathers results.</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="n">results</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="a-simplified-approach">
<h3><span class="section-number">5.2.3. </span>A simplified approach<a class="headerlink" href="#a-simplified-approach" title="Permalink to this heading"></a></h3>
<p>Transferring state to and from a Reactor can be complicated and add a lot of code. An alternative approach is to ensure
that the reactor state is synchronized across all nodes, and then use the reactor instead of raw data:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">SomeInterface</span><span class="p">(</span><span class="n">interfaces</span><span class="o">.</span><span class="n">Interface</span><span class="p">):</span>

    <span class="k">def</span> <span class="nf">interactEveryNode</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">cycle</span><span class="p">,</span> <span class="n">node</span><span class="p">):</span>
        <span class="n">actions</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="c1"># pseudo code for getting a bunch of different actions</span>
        <span class="k">for</span> <span class="n">opt</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">cs</span><span class="p">[</span><span class="s1">&#39;someSetting&#39;</span><span class="p">]:</span>
            <span class="n">actions</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">factory</span><span class="p">(</span><span class="n">opt</span><span class="p">))</span>

        <span class="n">mpiActions</span><span class="o">.</span><span class="n">DistributeStateAction</span><span class="o">.</span><span class="n">invokeAsMaster</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">o</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">r</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cs</span><span class="p">)</span>
        <span class="n">results</span> <span class="o">=</span> <span class="n">mpiActions</span><span class="o">.</span><span class="n">runActions</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">o</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">r</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cs</span><span class="p">,</span> <span class="n">actions</span><span class="p">)</span>

<span class="k">class</span> <span class="nc">WhatAction</span><span class="p">(</span><span class="n">mpiActions</span><span class="o">.</span><span class="n">MpiAction</span><span class="p">):</span>

    <span class="k">def</span> <span class="nf">invokeHook</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>

        <span class="c1"># do something</span>
        <span class="k">for</span> <span class="n">a</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">generateMyObjects</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">r</span><span class="p">):</span>
            <span class="n">a</span><span class="o">.</span><span class="n">p</span><span class="o">.</span><span class="n">someParam</span> <span class="o">=</span> <span class="n">func</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">b</span> <span class="ow">in</span> <span class="n">a</span><span class="p">:</span>
                <span class="n">b</span><span class="o">.</span><span class="n">p</span><span class="o">.</span><span class="n">someParam</span> <span class="o">=</span> <span class="n">func</span><span class="p">(</span><span class="n">b</span><span class="p">)</span>

        <span class="c1"># notice we don&#39;t return an value, but instead just sync the state,</span>
        <span class="c1"># which updates the primary node with the params that the workers changed.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">r</span><span class="o">.</span><span class="n">syncMpiState</span><span class="p">()</span>
</pre></div>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Only parameters that are set are synchronized to the primary node. Consequently if a mutable
parameter (e.g. <code class="docutils literal notranslate"><span class="pre">b.p.depletionMatrix</span></code> which is of type <code class="docutils literal notranslate"><span class="pre">BurnMatrix</span></code>) is changed, it will
not natively be synced. To flag it to be synced, <code class="docutils literal notranslate"><span class="pre">b.p.paramName</span></code> must be set, even if it is
to the same object. For this reason, setting parameters to mutable objects should be avoided.
Further, if the mutable object has a reference to a large object, such as a composite or
cross section library, it can be very computationally expensive to pass all this data to the primary node.
See also: <a class="reference internal" href="../.apidocs/armi.reactor.parameters.html#module-armi.reactor.parameters" title="armi.reactor.parameters"><code class="xref py py-mod docutils literal notranslate"><span class="pre">armi.reactor.parameters</span></code></a></p>
</div>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="documenting.html" class="btn btn-neutral float-left" title="4. Documenting ARMI" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="profiling.html" class="btn btn-neutral float-right" title="6. Profiling ARMI" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2009-2024, TerraPower, LLC.
      <span class="lastupdated">Last updated on 2024-04-05.
      </span></p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>